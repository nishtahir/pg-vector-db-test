{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --quiet InstructorEmbedding ipywidgets langchain psycopg2-binary python-dotenv scikit-learn sentence_transformers tqdm   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection established successfully!\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "\n",
    "try:\n",
    "    # todo move to env vars\n",
    "    connection = psycopg2.connect(\n",
    "        os.environ[\"DATABASE_URL\"]\n",
    "    )\n",
    "    print(\"Connection established successfully!\")\n",
    "except psycopg2.Error as e:\n",
    "    print(f\"Error connecting to PostgreSQL: {e}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the database by creating the required table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with connection.cursor() as cursor:\n",
    "    cursor.execute(\"DROP TABLE IF EXISTS dataset;\")\n",
    "    cursor.execute(\"CREATE TABLE dataset(id SERIAL PRIMARY KEY, text TEXT, embeddings DOUBLE PRECISION[]);\")\n",
    "    connection.commit()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a similarity function. I'm using cosine similarity here. I'm not sure if that's the best choice, but it's a good starting point. As a reference point, I'll be creating my own similarity function, we can meausre against built in functions in pgvector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with connection.cursor() as cursor:\n",
    "    cursor.execute(\"DROP FUNCTION IF EXISTS cosine_distance(a DOUBLE PRECISION[], b DOUBLE PRECISION[]);\")\n",
    "    cursor.execute(\"\"\"\n",
    "        CREATE FUNCTION cosine_distance(a DOUBLE PRECISION[], b DOUBLE PRECISION[]) RETURNS DOUBLE PRECISION AS $$\n",
    "        DECLARE\n",
    "            dot DOUBLE PRECISION := 0;\n",
    "            mag_a DOUBLE PRECISION := 0;\n",
    "            mag_b DOUBLE PRECISION := 0;\n",
    "            i INTEGER := 1;\n",
    "        BEGIN\n",
    "            WHILE i <= array_length(a, 1) LOOP\n",
    "                dot := dot + a[i] * b[i];\n",
    "                mag_a := mag_a + a[i] * a[i];\n",
    "                mag_b := mag_b + b[i] * b[i];\n",
    "                i := i + 1;\n",
    "            END LOOP;\n",
    "            RETURN 1 - (dot / sqrt(mag_a * mag_b));\n",
    "        END;\n",
    "        $$ LANGUAGE plpgsql;\n",
    "\n",
    "    \"\"\")\n",
    "    connection.commit()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need data to embed. I'm using a small corpus of the top 5 books from the Gutenberg project. The data is in the data/ directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus length: 26776\n"
     ]
    }
   ],
   "source": [
    "# read the corpus\n",
    "with open(\"data/gutenberg-top-5.txt\", \"r\") as f:\n",
    "    corpus = f.read()\n",
    "    # split into paragraphs\n",
    "    corpus = corpus.split(\"\\n\\n\")\n",
    "    # remove blank lines\n",
    "    corpus = [line for line in corpus if line.strip() != \"\"]\n",
    "    print(f\"Corpus length: {len(corpus)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to create the embeddings for the data. I'm using the `HuggingFaceInstructEmbeddings` model here not because it's the best, but because I can run it locally. Sine I'm on  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nishtahir/Developer/postgres-vector-db-test/.venv/lib/python3.10/site-packages/InstructorEmbedding/instructor.py:7: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "import torch\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps:0\")\n",
    "else:\n",
    "    print(\"MPS device not found.\")\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "hf = HuggingFaceInstructEmbeddings(\n",
    "    model_name=\"hkunlp/instructor-large\",\n",
    "    model_kwargs={'device': device},\n",
    "    encode_kwargs={'normalize_embeddings': True}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9db0560fad5d4d0ebebfca6319ab0ad4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nishtahir/Developer/postgres-vector-db-test/.venv/lib/python3.10/site-packages/InstructorEmbedding/instructor.py:278: UserWarning: MPS: no support for int64 reduction ops, casting it to int32 (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/mps/operations/ReduceOps.mm:144.)\n",
      "  assert torch.sum(attention_mask[local_idx]).item() >= context_masks[local_idx].item(),\\\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "with connection.cursor() as cursor:\n",
    "    for line in tqdm(corpus[:1000]):\n",
    "        embedding = hf.embed_documents(line)[0]\n",
    "        cursor.execute(\"INSERT INTO dataset(text, embeddings) VALUES (%s, %s)\", (line, embedding))\n",
    "        connection.commit()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we want to figure out here is how fast we can retrieve similarly rated content for given text.\n",
    "* What sort of indexes do we need to create to make this fast?\n",
    "* How fast can we make it?\n",
    "* Does partitioning the data help?\n",
    "* How much does the pgvector extension help?\n",
    "* Does clustering the embeddings help"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm using embeddings from a random query here as an example. In an ideal usecase you should be creating your own embeddings from your query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_query = f\"\"\"\n",
    "    WITH target_item AS (\n",
    "        SELECT * \n",
    "        FROM dataset \n",
    "        WHERE id=60\n",
    "    )\n",
    "    SELECT ds.text, cosine_distance(ds.embeddings, target_item.embeddings) as distance\n",
    "    FROM dataset ds, target_item \n",
    "    ORDER BY distance DESC \n",
    "    LIMIT 10;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24268183173368: CAPULET. Go to, go to! You are a saucy boy. Is’t so, indeed? This trick may chan\n",
      "0.24268183173368: CAPULET. He shall be endur’d. What, goodman boy! I say he shall, go to; Am I the\n",
      "0.24268183173368: CAPULET. And too soon marr’d are those so early made. The earth hath swallowed a\n",
      "0.24268183173368: CAPULET. But Montague is bound as well as I, In penalty alike; and ’tis not hard\n",
      "0.24268183173368: CHORUS. Two households, both alike in dignity, In fair Verona, where we lay our \n",
      "0.24268183173368: CAPULET. My sword, I say! Old Montague is come, And flourishes his blade in spit\n",
      "0.24268183173368: CAPULET. But saying o’er what I have said before. My child is yet a stranger in \n",
      "0.24268183173368: CAPULET, head of a Veronese family at feud with the Montagues. LADY CAPULET, wif\n",
      "0.24268183173368: CAPULET. What noise is this? Give me my long sword, ho!\n",
      "0.24268183173368: CAPULET. Welcome, gentlemen, ladies that have their toes Unplagu’d with corns wi\n"
     ]
    }
   ],
   "source": [
    "with connection.cursor() as cursor:\n",
    "    cursor.execute(test_query)\n",
    "    for line in cursor.fetchall():\n",
    "        text = line[0].strip().replace(\"\\n\", \" \")[:80]\n",
    "        score = line[1]\n",
    "        print(f\"{score}: {text}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting distances aren't the best looking however there is some resemblance in the text. Let's analyze the query plan to see what's going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Limit  (cost=319.73..319.78 rows=20 width=135) (actual time=581.897..581.901 rows=20 loops=1)\n",
      "  ->  Sort  (cost=319.73..322.21 rows=992 width=135) (actual time=581.896..581.897 rows=20 loops=1)\n",
      "        Sort Key: (cosine_distance(ds.embeddings, dataset.embeddings)) DESC\n",
      "        Sort Method: top-N heapsort  Memory: 36kB\n",
      "        ->  Nested Loop  (cost=0.28..293.33 rows=992 width=135) (actual time=0.644..581.407 rows=1000 loops=1)\n",
      "              ->  Index Scan using dataset_pkey on dataset  (cost=0.28..2.49 rows=1 width=18) (actual time=0.012..0.013 rows=1 loops=1)\n",
      "                    Index Cond: (id = 60)\n",
      "              ->  Seq Scan on dataset ds  (cost=0.00..32.92 rows=992 width=145) (actual time=0.010..0.270 rows=1000 loops=1)\n",
      "Planning Time: 0.114 ms\n",
      "Execution Time: 581.946 ms\n"
     ]
    }
   ],
   "source": [
    "with connection.cursor() as cursor:\n",
    "    cursor.execute(\"EXPLAIN ANALYZE \" + test_query)\n",
    "    for line in cursor.fetchall():\n",
    "        print(line[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add an index to the embeddings column and see if that helps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with connection.cursor() as cursor:\n",
    "    cursor.execute(\"DROP TABLE IF EXISTS indexed_dataset;\")\n",
    "    cursor.execute(\"DROP INDEX IF EXISTS indexed_dataset_embeddings_idx;\")\n",
    "\n",
    "    cursor.execute(\"CREATE TABLE indexed_dataset AS SELECT * from dataset;\")\n",
    "    cursor.execute(\"CREATE INDEX indexed_dataset_embeddings_idx ON indexed_dataset USING GIN(embeddings);\")\n",
    "    \n",
    "    connection.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_indexed_query = f\"\"\"\n",
    "  WITH target_item AS (\n",
    "      SELECT * \n",
    "      FROM indexed_dataset \n",
    "      WHERE id=60\n",
    "  )\n",
    "  SELECT ds.text, cosine_distance(ds.embeddings, target_item.embeddings) as distance\n",
    "  FROM indexed_dataset ds, target_item \n",
    "  ORDER BY distance DESC \n",
    "  LIMIT 20;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Limit  (cost=355.11..355.16 rows=20 width=134) (actual time=591.074..591.078 rows=20 loops=1)\n",
      "  ->  Sort  (cost=355.11..357.61 rows=1000 width=134) (actual time=591.073..591.074 rows=20 loops=1)\n",
      "        Sort Key: (cosine_distance(ds.embeddings, indexed_dataset.embeddings)) DESC\n",
      "        Sort Method: top-N heapsort  Memory: 36kB\n",
      "        ->  Nested Loop  (cost=0.00..328.50 rows=1000 width=134) (actual time=1.624..590.544 rows=1000 loops=1)\n",
      "              ->  Seq Scan on indexed_dataset  (cost=0.00..35.50 rows=1 width=18) (actual time=0.060..0.184 rows=1 loops=1)\n",
      "                    Filter: (id = 60)\n",
      "                    Rows Removed by Filter: 999\n",
      "              ->  Seq Scan on indexed_dataset ds  (cost=0.00..33.00 rows=1000 width=144) (actual time=0.002..0.336 rows=1000 loops=1)\n",
      "Planning Time: 0.860 ms\n",
      "Execution Time: 591.280 ms\n"
     ]
    }
   ],
   "source": [
    "with connection.cursor() as cursor:\n",
    "    cursor.execute(\"EXPLAIN ANALYZE \" + test_indexed_query)\n",
    "    for line in cursor.fetchall():\n",
    "        print(line[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like having an index doesn't improve the query all that much because it needs to scan the entire table to to compute the distances.\n",
    "One way to mitigate this is to cluster the table on the embeddings column. This will group similar embeddings together on disk and make it faster to scan the table. We can't use the `CLUSTER` feature because it's not supported on GIN indexes, however we can pre-compute the clusters using a K-Means clustering algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with connection.cursor() as cursor:\n",
    "    cursor.execute(\"SELECT embeddings FROM dataset;\")\n",
    "    embeddings = cursor.fetchall()\n",
    "    embeddings = [e[0] for e in embeddings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': 564, '1': 94, '2': 159, '3': 116, '4': 67}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=5, random_state=0, n_init=\"auto\").fit(embeddings)\n",
    "cluster_data = {\n",
    "    'clusters': kmeans.labels_.tolist(),\n",
    "    'centroids': kmeans.cluster_centers_.tolist(),\n",
    "    'iterations': kmeans.n_iter_,\n",
    "    'inertia': kmeans.inertia_,\n",
    "    'converged': kmeans.n_iter_ < kmeans.max_iter,\n",
    "    'counts': {\n",
    "        '0': (kmeans.labels_ == 0).sum(),\n",
    "        '1': (kmeans.labels_ == 1).sum(),\n",
    "        '2': (kmeans.labels_ == 2).sum(),\n",
    "        '3': (kmeans.labels_ == 3).sum(),\n",
    "        '4': (kmeans.labels_ == 4).sum(),\n",
    "    }\n",
    "}\n",
    "\n",
    "print(cluster_data['counts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with connection.cursor() as cursor:\n",
    "    cursor.execute(\"DROP TABLE IF EXISTS clusters;\")\n",
    "    cursor.execute(\"CREATE TABLE clusters(id INTEGER, centroid double precision[]);\")\n",
    "    for i, centroid in enumerate(cluster_data['centroids']):\n",
    "        cursor.execute(\"INSERT INTO clusters(id, centroid) VALUES (%s, %s)\", (i, centroid))\n",
    "    connection.commit()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a cluster map we can add a cluster index to the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with connection.cursor() as cursor:\n",
    "    cursor.execute(\"DROP TABLE IF EXISTS clustered_dataset;\")\n",
    "    cursor.execute(\"CREATE TABLE clustered_dataset AS SELECT * from dataset;\")\n",
    "    cursor.execute(\"ALTER TABLE clustered_dataset ADD COLUMN cluster INTEGER;\")\n",
    "\n",
    "    for i, cluster in enumerate(cluster_data['clusters']):\n",
    "        cursor.execute(\"UPDATE clustered_dataset SET cluster=%s WHERE id=%s\", (cluster, i+1))\n",
    "\n",
    "    connection.commit()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now querying data happens in 2 steps\n",
    "1. Find the cluster that the query text belongs to\n",
    "2. Find the nearest neighbors within that cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered_test_query = \"\"\"\n",
    "    EXPLAIN ANALYZE WITH target_item AS (\n",
    "    SELECT id, embeddings, cluster\n",
    "    FROM clustered_dataset\n",
    "    WHERE id = 60\n",
    "    ),\n",
    "    nearest_centroid AS (\n",
    "    SELECT c.id AS centroid_id, c.centroid\n",
    "    FROM clusters c\n",
    "    ORDER BY cosine_distance(c.centroid, (SELECT embeddings FROM target_item)) DESC\n",
    "    LIMIT 1\n",
    "    )\n",
    "    SELECT ds.id, ds.text\n",
    "    FROM clustered_dataset ds\n",
    "    WHERE ds.cluster = (SELECT centroid_id FROM nearest_centroid)\n",
    "    AND ds.id <> 60\n",
    "    ORDER BY cosine_distance(ds.embeddings, (SELECT embeddings FROM target_item)) DESC\n",
    "    LIMIT 20;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Limit  (cost=519.43..519.48 rows=20 width=138) (actual time=96.114..96.122 rows=20 loops=1)\n",
      "  CTE target_item\n",
      "    ->  Seq Scan on clustered_dataset  (cost=0.00..57.50 rows=1 width=26) (actual time=0.364..0.510 rows=1 loops=1)\n",
      "          Filter: (id = 60)\n",
      "          Rows Removed by Filter: 999\n",
      "  CTE nearest_centroid\n",
      "    ->  Limit  (cost=346.57..346.57 rows=1 width=44) (actual time=4.475..4.477 rows=1 loops=1)\n",
      "          InitPlan 2 (returns $1)\n",
      "            ->  CTE Scan on target_item  (cost=0.00..0.02 rows=1 width=32) (actual time=0.366..0.512 rows=1 loops=1)\n",
      "          ->  Sort  (cost=346.55..349.73 rows=1270 width=44) (actual time=4.474..4.475 rows=1 loops=1)\n",
      "                Sort Key: (cosine_distance(c.centroid, $1)) DESC\n",
      "                Sort Method: top-N heapsort  Memory: 25kB\n",
      "                ->  Seq Scan on clusters c  (cost=0.00..340.20 rows=1270 width=44) (actual time=2.169..4.463 rows=5 loops=1)\n",
      "  InitPlan 4 (returns $3)\n",
      "    ->  CTE Scan on target_item target_item_1  (cost=0.00..0.02 rows=1 width=32) (actual time=0.001..0.002 rows=1 loops=1)\n",
      "  InitPlan 5 (returns $4)\n",
      "    ->  CTE Scan on nearest_centroid  (cost=0.00..0.02 rows=1 width=4) (actual time=4.478..4.479 rows=1 loops=1)\n",
      "  ->  Sort  (cost=115.32..115.82 rows=200 width=138) (actual time=96.112..96.114 rows=20 loops=1)\n",
      "        Sort Key: (cosine_distance(ds.embeddings, $3)) DESC\n",
      "        Sort Method: top-N heapsort  Memory: 28kB\n",
      "        ->  Seq Scan on clustered_dataset ds  (cost=0.00..110.00 rows=200 width=138) (actual time=5.078..96.005 rows=159 loops=1)\n",
      "              Filter: ((id <> 60) AND (cluster = $4))\n",
      "              Rows Removed by Filter: 841\n",
      "Planning Time: 0.639 ms\n",
      "Execution Time: 96.342 ms\n"
     ]
    }
   ],
   "source": [
    "with connection.cursor() as cursor:\n",
    "    cursor.execute(clustered_test_query)\n",
    "    for line in cursor.fetchall():\n",
    "        print(line[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This has greatly improved the performance. We've been able to eliminate the need to scan the entire table (it's only looking at 563 rows). The performance here will be impacted by the number of clusters you are able to generate. however this does mean that you may be excluding results that are somewhat close but in neibooring clusters. One approach to mitigate this is to query the nearest neighbors from the neighboring clusters as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "with connection.cursor() as cursor:\n",
    "    cursor.execute(\"DROP INDEX IF EXISTS clustered_dataset_pkey;\")\n",
    "    cursor.execute(\"DROP INDEX IF EXISTS idx_clustered_dataset_cluster;\")\n",
    "    cursor.execute(\"DROP INDEX IF EXISTS idx_clusters_centroid;\")\n",
    "    cursor.execute(\"CREATE UNIQUE INDEX clustered_dataset_pkey ON clustered_dataset(id int4_ops);\")\n",
    "    cursor.execute(\"CREATE INDEX idx_clustered_dataset_cluster ON clustered_dataset (cluster);\")\n",
    "    cursor.execute(\"CREATE INDEX idx_clusters_centroid ON clusters USING GIN (centroid);\")\n",
    "    connection.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Limit  (cost=104.43..104.48 rows=20 width=138) (actual time=97.251..97.258 rows=20 loops=1)\n",
      "  CTE target_item\n",
      "    ->  Index Scan using clustered_dataset_pkey on clustered_dataset  (cost=0.28..2.49 rows=1 width=26) (actual time=0.031..0.032 rows=1 loops=1)\n",
      "          Index Cond: (id = 60)\n",
      "  CTE nearest_centroid\n",
      "    ->  Limit  (cost=2.34..2.35 rows=1 width=44) (actual time=3.091..3.093 rows=1 loops=1)\n",
      "          InitPlan 2 (returns $1)\n",
      "            ->  CTE Scan on target_item  (cost=0.00..0.02 rows=1 width=32) (actual time=0.033..0.034 rows=1 loops=1)\n",
      "          ->  Sort  (cost=2.32..2.34 rows=5 width=44) (actual time=3.090..3.091 rows=1 loops=1)\n",
      "                Sort Key: (cosine_distance(c.centroid, $1)) DESC\n",
      "                Sort Method: top-N heapsort  Memory: 25kB\n",
      "                ->  Seq Scan on clusters c  (cost=0.00..2.30 rows=5 width=44) (actual time=0.684..3.082 rows=5 loops=1)\n",
      "  InitPlan 4 (returns $3)\n",
      "    ->  CTE Scan on target_item target_item_1  (cost=0.00..0.02 rows=1 width=32) (actual time=0.001..0.002 rows=1 loops=1)\n",
      "  InitPlan 5 (returns $4)\n",
      "    ->  CTE Scan on nearest_centroid  (cost=0.00..0.02 rows=1 width=4) (actual time=3.093..3.093 rows=1 loops=1)\n",
      "  ->  Sort  (cost=99.55..100.05 rows=200 width=138) (actual time=97.249..97.251 rows=20 loops=1)\n",
      "        Sort Key: (cosine_distance(ds.embeddings, $3)) DESC\n",
      "        Sort Method: top-N heapsort  Memory: 28kB\n",
      "        ->  Index Scan using idx_clustered_dataset_cluster on clustered_dataset ds  (cost=0.15..94.23 rows=200 width=138) (actual time=3.748..97.148 rows=159 loops=1)\n",
      "              Index Cond: (cluster = $4)\n",
      "              Filter: (id <> 60)\n",
      "Planning Time: 0.392 ms\n",
      "Execution Time: 97.332 ms\n"
     ]
    }
   ],
   "source": [
    "with connection.cursor() as cursor:\n",
    "    cursor.execute(clustered_test_query)\n",
    "    for line in cursor.fetchall():\n",
    "        print(line[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding indexes allows us to replace some sequential scans with index scans. This isn't that big of an improvement here but would be more significant with a larger dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the pgvector extension to create a GIN index on the embeddings column. This will allow us to use the `pgvector <-> pgvector` operator to find the nearest neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "with connection.cursor() as cursor:\n",
    "    cursor.execute(\"DROP TABLE IF EXISTS vectorized_dataset;\")\n",
    "    cursor.execute(\"CREATE EXTENSION IF NOT EXISTS vector WITH SCHEMA extensions;\");\n",
    "    cursor.execute(\"CREATE TABLE vectorized_dataset AS SELECT * from dataset;\")\n",
    "    \n",
    "    # 768 seems to be the default length for the instructor-large model\n",
    "    cursor.execute(\"ALTER TABLE vectorized_dataset ADD COLUMN embeddings_vector vector(768);\")\n",
    "    connection.commit()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's copy over our data to the new table and see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "with connection.cursor() as cursor:\n",
    "    cursor.execute(\"SELECT embeddings FROM vectorized_dataset;\")\n",
    "    embeddings = cursor.fetchall()\n",
    "    embeddings = [e[0] for e in embeddings]\n",
    "    for i, embedding in enumerate(embeddings):\n",
    "        cursor.execute(\"UPDATE vectorized_dataset SET embeddings_vector=%s WHERE id=%s\", (embedding, i+1))\n",
    "    connection.commit()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_test_query = \"\"\"\n",
    "    WITH target_item AS (\n",
    "        SELECT * \n",
    "        FROM vectorized_dataset \n",
    "        WHERE id=60\n",
    "    )\n",
    "    SELECT ds.text, ds.embeddings_vector <=> target_item.embeddings_vector as score\n",
    "    FROM vectorized_dataset ds, target_item\n",
    "    ORDER BY score DESC\n",
    "    LIMIT 10;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.242681830496018: BENVOLIO. This wind you talk of blows us from ourselves: Supper is done, and we \n",
      "0.242681830496018: CAPULET. My sword, I say! Old Montague is come, And flourishes his blade in spit\n",
      "0.242681830496018: JULIET. And stint thou too, I pray thee, Nurse, say I.\n",
      "0.242681830496018: CAPULET. And too soon marr’d are those so early made. The earth hath swallowed a\n",
      "0.242681830496018: CAPULET. What noise is this? Give me my long sword, ho!\n",
      "0.242681830496018: CAPULET, head of a Veronese family at feud with the Montagues. LADY CAPULET, wif\n",
      "0.242681830496018: CHORUS. Two households, both alike in dignity, In fair Verona, where we lay our \n",
      "0.242681830496018: CAPULET. But Montague is bound as well as I, In penalty alike; and ’tis not hard\n",
      "0.242681830496018: CAPULET. But saying o’er what I have said before. My child is yet a stranger in \n",
      "0.242681830496018: CAPULET. What, man, ’tis not so much, ’tis not so much: ’Tis since the nuptial o\n"
     ]
    }
   ],
   "source": [
    "with connection.cursor() as cursor:\n",
    "    cursor.execute(vectorized_test_query)\n",
    "    for line in cursor.fetchall():\n",
    "        text = line[0].strip().replace(\"\\n\", \" \")[:80]\n",
    "        score = line[1]\n",
    "        print(f\"{score}: {text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Limit  (cost=155.61..155.66 rows=20 width=134) (actual time=7.986..7.991 rows=20 loops=1)\n",
      "  ->  Sort  (cost=155.61..158.11 rows=1000 width=134) (actual time=7.985..7.987 rows=20 loops=1)\n",
      "        Sort Key: ((ds.embeddings_vector <=> vectorized_dataset.embeddings_vector)) DESC\n",
      "        Sort Method: top-N heapsort  Memory: 35kB\n",
      "        ->  Nested Loop  (cost=0.00..129.00 rows=1000 width=134) (actual time=0.071..7.667 rows=1000 loops=1)\n",
      "              ->  Seq Scan on vectorized_dataset  (cost=0.00..59.50 rows=1 width=18) (actual time=0.044..0.144 rows=1 loops=1)\n",
      "                    Filter: (id = 60)\n",
      "                    Rows Removed by Filter: 999\n",
      "              ->  Seq Scan on vectorized_dataset ds  (cost=0.00..57.00 rows=1000 width=144) (actual time=0.002..0.119 rows=1000 loops=1)\n",
      "Planning Time: 0.096 ms\n",
      "Execution Time: 8.032 ms\n"
     ]
    }
   ],
   "source": [
    "with connection.cursor() as cursor:\n",
    "    cursor.execute(\"EXPLAIN ANALYZE\" + vectorized_test_query)\n",
    "    for line in cursor.fetchall():\n",
    "        print(line[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance difference is staggering. We've gone from 333.352 ms to 8.112 ms. That's a 41x improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "with connection.cursor() as cursor:\n",
    "    cursor.execute(\"DROP TABLE IF EXISTS indexed_vectorized_dataset;\")\n",
    "    cursor.execute(\"DROP INDEX IF EXISTS idx_indexed_vectorized_dataset_pkey;\")\n",
    "    cursor.execute(\n",
    "        \"DROP INDEX IF EXISTS idx_indexed_vectorized_dataset_embeddings_vector_idx;\"\n",
    "    )\n",
    "\n",
    "    cursor.execute(\n",
    "        \"CREATE TABLE indexed_vectorized_dataset AS SELECT * from vectorized_dataset;\")\n",
    "    cursor.execute(\n",
    "        \"CREATE INDEX idx_indexed_vectorized_dataset_pkey ON indexed_vectorized_dataset(id int4_ops);\")\n",
    "    cursor.execute(\n",
    "        \"CREATE INDEX idx_indexed_vectorized_dataset_embeddings_vector ON indexed_vectorized_dataset USING ivfflat (embeddings_vector vector_cosine_ops) WITH (lists = 100);\"\n",
    "    )\n",
    "\n",
    "    connection.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexed_vectorized_test_query = \"\"\"\n",
    "    WITH target_item AS (\n",
    "        SELECT * \n",
    "        FROM indexed_vectorized_dataset \n",
    "        WHERE id=60\n",
    "    )\n",
    "    SELECT ds.text, ds.embeddings_vector <=> target_item.embeddings_vector as score\n",
    "    FROM indexed_vectorized_dataset ds, target_item\n",
    "    ORDER BY score DESC\n",
    "    LIMIT 20;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Limit  (cost=10000000076.60..10000000076.65 rows=20 width=134) (actual time=8.033..8.036 rows=20 loops=1)\n",
      "  ->  Sort  (cost=10000000076.60..10000000079.10 rows=1000 width=134) (actual time=8.031..8.033 rows=20 loops=1)\n",
      "        Sort Key: ((ds.embeddings_vector <=> indexed_vectorized_dataset.embeddings_vector)) DESC\n",
      "        Sort Method: top-N heapsort  Memory: 35kB\n",
      "        ->  Nested Loop  (cost=10000000000.27..10000000049.99 rows=1000 width=134) (actual time=0.045..7.734 rows=1000 loops=1)\n",
      "              ->  Index Scan using idx_indexed_vectorized_dataset_pkey on indexed_vectorized_dataset  (cost=0.28..2.49 rows=1 width=18) (actual time=0.012..0.013 rows=1 loops=1)\n",
      "                    Index Cond: (id = 60)\n",
      "              ->  Seq Scan on indexed_vectorized_dataset ds  (cost=10000000000.00..10000000035.00 rows=1000 width=144) (actual time=0.008..0.192 rows=1000 loops=1)\n",
      "Planning Time: 0.117 ms\n",
      "Execution Time: 8.080 ms\n"
     ]
    }
   ],
   "source": [
    "with connection.cursor() as cursor:\n",
    "    cursor.execute(\"SET enable_seqscan=false;\")\n",
    "    connection.commit()\n",
    "    cursor.execute(\"EXPLAIN ANALYZE\" + indexed_vectorized_test_query)\n",
    "    for line in cursor.fetchall():\n",
    "        print(line[0])\n",
    "    cursor.execute(\"SET enable_seqscan=true;\")\n",
    "    connection.commit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
