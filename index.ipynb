{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: InstructorEmbedding in ./.venv/lib/python3.10/site-packages (1.0.1)\n",
      "Requirement already satisfied: ipywidgets in ./.venv/lib/python3.10/site-packages (8.0.6)\n",
      "Requirement already satisfied: langchain in ./.venv/lib/python3.10/site-packages (0.0.220)\n",
      "Requirement already satisfied: psycopg2-binary in ./.venv/lib/python3.10/site-packages (2.9.6)\n",
      "Collecting python-dotenv\n",
      "  Using cached python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: scikit-learn in ./.venv/lib/python3.10/site-packages (1.3.0)\n",
      "Requirement already satisfied: sentence_transformers in ./.venv/lib/python3.10/site-packages (2.2.2)\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.10/site-packages (4.65.0)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in ./.venv/lib/python3.10/site-packages (from ipywidgets) (6.23.3)\n",
      "Requirement already satisfied: ipython>=6.1.0 in ./.venv/lib/python3.10/site-packages (from ipywidgets) (8.14.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in ./.venv/lib/python3.10/site-packages (from ipywidgets) (5.9.0)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.7 in ./.venv/lib/python3.10/site-packages (from ipywidgets) (4.0.7)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.7 in ./.venv/lib/python3.10/site-packages (from ipywidgets) (3.0.7)\n",
      "Requirement already satisfied: PyYAML>=5.4.1 in ./.venv/lib/python3.10/site-packages (from langchain) (6.0)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in ./.venv/lib/python3.10/site-packages (from langchain) (2.0.17)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in ./.venv/lib/python3.10/site-packages (from langchain) (3.8.4)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in ./.venv/lib/python3.10/site-packages (from langchain) (4.0.2)\n",
      "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in ./.venv/lib/python3.10/site-packages (from langchain) (0.5.9)\n",
      "Requirement already satisfied: langchainplus-sdk>=0.0.17 in ./.venv/lib/python3.10/site-packages (from langchain) (0.0.19)\n",
      "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in ./.venv/lib/python3.10/site-packages (from langchain) (2.8.4)\n",
      "Requirement already satisfied: numpy<2,>=1 in ./.venv/lib/python3.10/site-packages (from langchain) (1.25.0)\n",
      "Requirement already satisfied: openapi-schema-pydantic<2.0,>=1.2 in ./.venv/lib/python3.10/site-packages (from langchain) (1.2.4)\n",
      "Requirement already satisfied: pydantic<2,>=1 in ./.venv/lib/python3.10/site-packages (from langchain) (1.10.10)\n",
      "Requirement already satisfied: requests<3,>=2 in ./.venv/lib/python3.10/site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in ./.venv/lib/python3.10/site-packages (from langchain) (8.2.2)\n",
      "Requirement already satisfied: scipy>=1.5.0 in ./.venv/lib/python3.10/site-packages (from scikit-learn) (1.11.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in ./.venv/lib/python3.10/site-packages (from scikit-learn) (1.3.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in ./.venv/lib/python3.10/site-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in ./.venv/lib/python3.10/site-packages (from sentence_transformers) (4.30.2)\n",
      "Requirement already satisfied: torch>=1.6.0 in ./.venv/lib/python3.10/site-packages (from sentence_transformers) (2.0.1)\n",
      "Requirement already satisfied: torchvision in ./.venv/lib/python3.10/site-packages (from sentence_transformers) (0.15.2)\n",
      "Requirement already satisfied: nltk in ./.venv/lib/python3.10/site-packages (from sentence_transformers) (3.8.1)\n",
      "Requirement already satisfied: sentencepiece in ./.venv/lib/python3.10/site-packages (from sentence_transformers) (0.1.99)\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in ./.venv/lib/python3.10/site-packages (from sentence_transformers) (0.15.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in ./.venv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./.venv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.venv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.3.0 in ./.venv/lib/python3.10/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (3.19.0)\n",
      "Requirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in ./.venv/lib/python3.10/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (1.5.1)\n",
      "Requirement already satisfied: typing-inspect>=0.4.0 in ./.venv/lib/python3.10/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (3.12.2)\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.7.0)\n",
      "Requirement already satisfied: packaging>=20.9 in ./.venv/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (23.1)\n",
      "Requirement already satisfied: appnope in ./.venv/lib/python3.10/site-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.3)\n",
      "Requirement already satisfied: comm>=0.1.1 in ./.venv/lib/python3.10/site-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.3)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in ./.venv/lib/python3.10/site-packages (from ipykernel>=4.5.1->ipywidgets) (1.6.7)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in ./.venv/lib/python3.10/site-packages (from ipykernel>=4.5.1->ipywidgets) (8.3.0)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in ./.venv/lib/python3.10/site-packages (from ipykernel>=4.5.1->ipywidgets) (5.3.1)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in ./.venv/lib/python3.10/site-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: nest-asyncio in ./.venv/lib/python3.10/site-packages (from ipykernel>=4.5.1->ipywidgets) (1.5.6)\n",
      "Requirement already satisfied: psutil in ./.venv/lib/python3.10/site-packages (from ipykernel>=4.5.1->ipywidgets) (5.9.5)\n",
      "Requirement already satisfied: pyzmq>=20 in ./.venv/lib/python3.10/site-packages (from ipykernel>=4.5.1->ipywidgets) (25.1.0)\n",
      "Requirement already satisfied: tornado>=6.1 in ./.venv/lib/python3.10/site-packages (from ipykernel>=4.5.1->ipywidgets) (6.3.2)\n",
      "Requirement already satisfied: backcall in ./.venv/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: decorator in ./.venv/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in ./.venv/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.18.2)\n",
      "Requirement already satisfied: pickleshare in ./.venv/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in ./.venv/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.38)\n",
      "Requirement already satisfied: pygments>=2.4.0 in ./.venv/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (2.15.1)\n",
      "Requirement already satisfied: stack-data in ./.venv/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.2)\n",
      "Requirement already satisfied: pexpect>4.3 in ./.venv/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2.0.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2023.5.7)\n",
      "Requirement already satisfied: sympy in ./.venv/lib/python3.10/site-packages (from torch>=1.6.0->sentence_transformers) (1.12)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.10/site-packages (from torch>=1.6.0->sentence_transformers) (3.1)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.10/site-packages (from torch>=1.6.0->sentence_transformers) (3.1.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2023.6.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in ./.venv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in ./.venv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.3.1)\n",
      "Requirement already satisfied: click in ./.venv/lib/python3.10/site-packages (from nltk->sentence_transformers) (8.1.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./.venv/lib/python3.10/site-packages (from torchvision->sentence_transformers) (10.0.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in ./.venv/lib/python3.10/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.10/site-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (2.8.2)\n",
      "Requirement already satisfied: platformdirs>=2.5 in ./.venv/lib/python3.10/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel>=4.5.1->ipywidgets) (3.8.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in ./.venv/lib/python3.10/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in ./.venv/lib/python3.10/site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=6.1.0->ipywidgets) (0.2.6)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in ./.venv/lib/python3.10/site-packages (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (1.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.10/site-packages (from jinja2->torch>=1.6.0->sentence_transformers) (2.1.3)\n",
      "Requirement already satisfied: executing>=1.2.0 in ./.venv/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (1.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in ./.venv/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.2.1)\n",
      "Requirement already satisfied: pure-eval in ./.venv/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in ./.venv/lib/python3.10/site-packages (from sympy->torch>=1.6.0->sentence_transformers) (1.3.0)\n",
      "Requirement already satisfied: six in ./.venv/lib/python3.10/site-packages (from asttokens>=2.1.0->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n",
      "Installing collected packages: python-dotenv\n",
      "Successfully installed python-dotenv-1.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install InstructorEmbedding ipywidgets langchain psycopg2-binary python-dotenv scikit-learn sentence_transformers tqdm  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection established successfully!\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "\n",
    "try:\n",
    "    # todo move to env vars\n",
    "    connection = psycopg2.connect(\n",
    "        os.environ[\"DATABASE_URL\"]\n",
    "    )\n",
    "    print(\"Connection established successfully!\")\n",
    "except psycopg2.Error as e:\n",
    "    print(f\"Error connecting to PostgreSQL: {e}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the database by creating the required table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with connection.cursor() as cursor:\n",
    "    cursor.execute(\"DROP TABLE IF EXISTS dataset;\")\n",
    "    cursor.execute(\"CREATE TABLE dataset(id SERIAL PRIMARY KEY, text TEXT, embeddings DOUBLE PRECISION[]);\")\n",
    "    connection.commit()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a similarity function. I'm using cosine similarity here. I'm not sure if that's the best choice, but it's a good starting point. As a reference point, I'll be creating my own similarity function, we can meausre against built in functions in pgvector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with connection.cursor() as cursor:\n",
    "    cursor.execute(\"DROP FUNCTION IF EXISTS cosine_distance(a DOUBLE PRECISION[], b DOUBLE PRECISION[]);\")\n",
    "    cursor.execute(\"\"\"\n",
    "        CREATE FUNCTION cosine_distance(a DOUBLE PRECISION[], b DOUBLE PRECISION[]) RETURNS DOUBLE PRECISION AS $$\n",
    "        DECLARE\n",
    "            dot DOUBLE PRECISION := 0;\n",
    "            mag_a DOUBLE PRECISION := 0;\n",
    "            mag_b DOUBLE PRECISION := 0;\n",
    "            i INTEGER := 1;\n",
    "        BEGIN\n",
    "            WHILE i <= array_length(a, 1) LOOP\n",
    "                dot := dot + a[i] * b[i];\n",
    "                mag_a := mag_a + a[i] * a[i];\n",
    "                mag_b := mag_b + b[i] * b[i];\n",
    "                i := i + 1;\n",
    "            END LOOP;\n",
    "            RETURN 1 - (dot / sqrt(mag_a * mag_b));\n",
    "        END;\n",
    "        $$ LANGUAGE plpgsql;\n",
    "\n",
    "    \"\"\")\n",
    "    connection.commit()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need data to embed. I'm using a small corpus of the top 5 books from the Gutenberg project. The data is in the data/ directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus length: 26776\n"
     ]
    }
   ],
   "source": [
    "# read the corpus\n",
    "with open(\"data/gutenberg-top-5.txt\", \"r\") as f:\n",
    "    corpus = f.read()\n",
    "    # split into paragraphs\n",
    "    corpus = corpus.split(\"\\n\\n\")\n",
    "    # remove blank lines\n",
    "    corpus = [line for line in corpus if line.strip() != \"\"]\n",
    "    print(f\"Corpus length: {len(corpus)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to create the embeddings for the data. I'm using the `HuggingFaceInstructEmbeddings` model here not because it's the best, but because I can run it locally. Sine I'm on  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nishtahir/Developer/postgres-vector-db-test/.venv/lib/python3.10/site-packages/InstructorEmbedding/instructor.py:7: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "import torch\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps:0\")\n",
    "else:\n",
    "    print(\"MPS device not found.\")\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "hf = HuggingFaceInstructEmbeddings(\n",
    "    model_name=\"hkunlp/instructor-large\",\n",
    "    model_kwargs={'device': device},\n",
    "    encode_kwargs={'normalize_embeddings': True}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9db0560fad5d4d0ebebfca6319ab0ad4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nishtahir/Developer/postgres-vector-db-test/.venv/lib/python3.10/site-packages/InstructorEmbedding/instructor.py:278: UserWarning: MPS: no support for int64 reduction ops, casting it to int32 (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/mps/operations/ReduceOps.mm:144.)\n",
      "  assert torch.sum(attention_mask[local_idx]).item() >= context_masks[local_idx].item(),\\\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "with connection.cursor() as cursor:\n",
    "    for line in tqdm(corpus[:1000]):\n",
    "        embedding = hf.embed_documents(line)[0]\n",
    "        cursor.execute(\"INSERT INTO dataset(text, embeddings) VALUES (%s, %s)\", (line, embedding))\n",
    "        connection.commit()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we want to figure out here is how fast we can retrieve similarly rated content for given text.\n",
    "* What sort of indexes do we need to create to make this fast?\n",
    "* How fast can we make it?\n",
    "* Does partitioning the data help?\n",
    "* How much does the pgvector extension help?\n",
    "* Does clustering the embeddings help"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm using embeddings from a random query here as an example. In an ideal usecase you should be creating your own embeddings from your query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_query = f\"\"\"\n",
    "    WITH target_item AS (\n",
    "        SELECT * \n",
    "        FROM dataset \n",
    "        WHERE id=60\n",
    "    )\n",
    "    SELECT ds.text, cosine_distance(ds.embeddings, target_item.embeddings) as distance\n",
    "    FROM dataset ds, target_item \n",
    "    ORDER BY distance DESC \n",
    "    LIMIT 20;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24268183173368: CHORUS. Now old desire doth in his deathbed lie, And young affection gapes to be his heir; That fair for which love groan’d for and would die, With tender Juliet match’d, is now not fair. Now Romeo is belov’d, and loves again, Alike bewitched by the charm of looks; But to his foe suppos’d he must complain, And she steal love’s sweet bait from fearful hooks: Being held a foe, he may not have access To breathe such vows as lovers use to swear; And she as much in love, her means much less To meet her new beloved anywhere. But passion lends them power, time means, to meet, Tempering extremities with extreme sweet.\n",
      "0.24268183173368: CAPULET. So many guests invite as here are writ.\n",
      "0.24268183173368: CAPULET. Why how now, kinsman! Wherefore storm you so?\n",
      "0.24268183173368: CAPULET. Will you tell me that? His son was but a ward two years ago.\n",
      "0.24268183173368: CAPULET. Nay, gentlemen, prepare not to be gone, We have a trifling foolish banquet towards. Is it e’en so? Why then, I thank you all; I thank you, honest gentlemen; good night. More torches here! Come on then, let’s to bed. Ah, sirrah, by my fay, it waxes late, I’ll to my rest.\n",
      "0.24268183173368: CAPULET. Content thee, gentle coz, let him alone, A bears him like a portly gentleman; And, to say truth, Verona brags of him To be a virtuous and well-govern’d youth. I would not for the wealth of all the town Here in my house do him disparagement. Therefore be patient, take no note of him, It is my will; the which if thou respect, Show a fair presence and put off these frowns, An ill-beseeming semblance for a feast.\n",
      "0.24268183173368: CAPULET. What, man, ’tis not so much, ’tis not so much: ’Tis since the nuptial of Lucentio, Come Pentecost as quickly as it will, Some five and twenty years; and then we mask’d.\n",
      "0.24268183173368: CAPULET. But Montague is bound as well as I, In penalty alike; and ’tis not hard, I think, For men so old as we to keep the peace.\n",
      "0.24268183173368: CAPULET. And too soon marr’d are those so early made. The earth hath swallowed all my hopes but she, She is the hopeful lady of my earth: But woo her, gentle Paris, get her heart, My will to her consent is but a part; And she agree, within her scope of choice Lies my consent and fair according voice. This night I hold an old accustom’d feast, Whereto I have invited many a guest, Such as I love, and you among the store, One more, most welcome, makes my number more. At my poor house look to behold this night Earth-treading stars that make dark heaven light: Such comfort as do lusty young men feel When well apparell’d April on the heel Of limping winter treads, even such delight Among fresh female buds shall you this night Inherit at my house. Hear all, all see, And like her most whose merit most shall be: Which, on more view of many, mine, being one, May stand in number, though in reckoning none. Come, go with me. Go, sirrah, trudge about Through fair Verona; find those persons out Whose names are written there, [_gives a paper_] and to them say, My house and welcome on their pleasure stay.\n",
      "0.24268183173368: CAPULET’S COUSIN. ’Tis more, ’tis more, his son is elder, sir; His son is thirty.\n",
      "0.24268183173368: CAPULET’S COUSIN. By’r Lady, thirty years.\n",
      "0.24268183173368: CAPULET. He shall be endur’d. What, goodman boy! I say he shall, go to; Am I the master here, or you? Go to. You’ll not endure him! God shall mend my soul, You’ll make a mutiny among my guests! You will set cock-a-hoop, you’ll be the man!\n",
      "0.24268183173368: CAPULET. Welcome, gentlemen, ladies that have their toes Unplagu’d with corns will have a bout with you. Ah my mistresses, which of you all Will now deny to dance? She that makes dainty, She I’ll swear hath corns. Am I come near ye now? Welcome, gentlemen! I have seen the day That I have worn a visor, and could tell A whispering tale in a fair lady’s ear, Such as would please; ’tis gone, ’tis gone, ’tis gone, You are welcome, gentlemen! Come, musicians, play. A hall, a hall, give room! And foot it, girls.\n",
      "0.24268183173368: CAPULET. Go to, go to! You are a saucy boy. Is’t so, indeed? This trick may chance to scathe you, I know what. You must contrary me! Marry, ’tis time. Well said, my hearts!—You are a princox; go: Be quiet, or—More light, more light!—For shame! I’ll make you quiet. What, cheerly, my hearts.\n",
      "0.24268183173368: CHORUS. Two households, both alike in dignity, In fair Verona, where we lay our scene, From ancient grudge break to new mutiny, Where civil blood makes civil hands unclean. From forth the fatal loins of these two foes A pair of star-cross’d lovers take their life; Whose misadventur’d piteous overthrows Doth with their death bury their parents’ strife. The fearful passage of their death-mark’d love, And the continuance of their parents’ rage, Which, but their children’s end, nought could remove, Is now the two hours’ traffic of our stage; The which, if you with patient ears attend, What here shall miss, our toil shall strive to mend.\n",
      "0.24268183173368: CAPULET, head of a Veronese family at feud with the Montagues. LADY CAPULET, wife to Capulet. JULIET, daughter to Capulet. TYBALT, nephew to Lady Capulet. CAPULET’S COUSIN, an old man. NURSE to Juliet. PETER, servant to Juliet’s Nurse. SAMPSON, servant to Capulet. GREGORY, servant to Capulet. Servants.\n",
      "0.24268183173368: CAPULET. My sword, I say! Old Montague is come, And flourishes his blade in spite of me.\n",
      "0.24268183173368: CAPULET. What noise is this? Give me my long sword, ho!\n",
      "0.24268183173368: CAPULET. But saying o’er what I have said before. My child is yet a stranger in the world, She hath not seen the change of fourteen years; Let two more summers wither in their pride Ere we may think her ripe to be a bride.\n",
      "0.24268183173368: CAPULET. Things have fallen out, sir, so unluckily That we have had no time to move our daughter. Look you, she lov’d her kinsman Tybalt dearly, And so did I. Well, we were born to die. ’Tis very late; she’ll not come down tonight. I promise you, but for your company, I would have been abed an hour ago.\n"
     ]
    }
   ],
   "source": [
    "with connection.cursor() as cursor:\n",
    "    cursor.execute(test_query)\n",
    "    for line in cursor.fetchall():\n",
    "        text = line[0].strip().replace(\"\\n\", \" \")\n",
    "        score = line[1]\n",
    "        print(f\"{score}: {text}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting distances aren't the best looking however there is some resemblance in the text. Let's analyze the query plan to see what's going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Limit  (cost=319.73..319.78 rows=20 width=135) (actual time=581.897..581.901 rows=20 loops=1)\n",
      "  ->  Sort  (cost=319.73..322.21 rows=992 width=135) (actual time=581.896..581.897 rows=20 loops=1)\n",
      "        Sort Key: (cosine_distance(ds.embeddings, dataset.embeddings)) DESC\n",
      "        Sort Method: top-N heapsort  Memory: 36kB\n",
      "        ->  Nested Loop  (cost=0.28..293.33 rows=992 width=135) (actual time=0.644..581.407 rows=1000 loops=1)\n",
      "              ->  Index Scan using dataset_pkey on dataset  (cost=0.28..2.49 rows=1 width=18) (actual time=0.012..0.013 rows=1 loops=1)\n",
      "                    Index Cond: (id = 60)\n",
      "              ->  Seq Scan on dataset ds  (cost=0.00..32.92 rows=992 width=145) (actual time=0.010..0.270 rows=1000 loops=1)\n",
      "Planning Time: 0.114 ms\n",
      "Execution Time: 581.946 ms\n"
     ]
    }
   ],
   "source": [
    "with connection.cursor() as cursor:\n",
    "    cursor.execute(\"EXPLAIN ANALYZE \" + test_query)\n",
    "    for line in cursor.fetchall():\n",
    "        print(line[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add an index to the embeddings column and see if that helps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with connection.cursor() as cursor:\n",
    "    cursor.execute(\"DROP TABLE IF EXISTS indexed_dataset;\")\n",
    "    cursor.execute(\"DROP INDEX IF EXISTS indexed_dataset_embeddings_idx;\")\n",
    "\n",
    "    cursor.execute(\"CREATE TABLE indexed_dataset AS SELECT * from dataset;\")\n",
    "    cursor.execute(\"CREATE INDEX indexed_dataset_embeddings_idx ON indexed_dataset USING GIN(embeddings);\")\n",
    "    \n",
    "    connection.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_indexed_query = f\"\"\"\n",
    "  WITH target_item AS (\n",
    "      SELECT * \n",
    "      FROM indexed_dataset \n",
    "      WHERE id=60\n",
    "  )\n",
    "  SELECT ds.text, cosine_distance(ds.embeddings, target_item.embeddings) as distance\n",
    "  FROM indexed_dataset ds, target_item \n",
    "  ORDER BY distance DESC \n",
    "  LIMIT 20;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Limit  (cost=355.11..355.16 rows=20 width=134) (actual time=591.074..591.078 rows=20 loops=1)\n",
      "  ->  Sort  (cost=355.11..357.61 rows=1000 width=134) (actual time=591.073..591.074 rows=20 loops=1)\n",
      "        Sort Key: (cosine_distance(ds.embeddings, indexed_dataset.embeddings)) DESC\n",
      "        Sort Method: top-N heapsort  Memory: 36kB\n",
      "        ->  Nested Loop  (cost=0.00..328.50 rows=1000 width=134) (actual time=1.624..590.544 rows=1000 loops=1)\n",
      "              ->  Seq Scan on indexed_dataset  (cost=0.00..35.50 rows=1 width=18) (actual time=0.060..0.184 rows=1 loops=1)\n",
      "                    Filter: (id = 60)\n",
      "                    Rows Removed by Filter: 999\n",
      "              ->  Seq Scan on indexed_dataset ds  (cost=0.00..33.00 rows=1000 width=144) (actual time=0.002..0.336 rows=1000 loops=1)\n",
      "Planning Time: 0.860 ms\n",
      "Execution Time: 591.280 ms\n"
     ]
    }
   ],
   "source": [
    "with connection.cursor() as cursor:\n",
    "    cursor.execute(\"EXPLAIN ANALYZE \" + test_indexed_query)\n",
    "    for line in cursor.fetchall():\n",
    "        print(line[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like having an index doesn't improve the query all that much because it needs to scan the entire table to to compute the distances.\n",
    "One way to mitigate this is to cluster the table on the embeddings column. This will group similar embeddings together on disk and make it faster to scan the table. We can't use the `CLUSTER` feature because it's not supported on GIN indexes, however we can pre-compute the clusters using a K-Means clustering algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with connection.cursor() as cursor:\n",
    "    cursor.execute(\"SELECT embeddings FROM dataset;\")\n",
    "    embeddings = cursor.fetchall()\n",
    "    embeddings = [e[0] for e in embeddings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': 564, '1': 94, '2': 159, '3': 116, '4': 67}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=5, random_state=0, n_init=\"auto\").fit(embeddings)\n",
    "cluster_data = {\n",
    "    'clusters': kmeans.labels_.tolist(),\n",
    "    'centroids': kmeans.cluster_centers_.tolist(),\n",
    "    'iterations': kmeans.n_iter_,\n",
    "    'inertia': kmeans.inertia_,\n",
    "    'converged': kmeans.n_iter_ < kmeans.max_iter,\n",
    "    'counts': {\n",
    "        '0': (kmeans.labels_ == 0).sum(),\n",
    "        '1': (kmeans.labels_ == 1).sum(),\n",
    "        '2': (kmeans.labels_ == 2).sum(),\n",
    "        '3': (kmeans.labels_ == 3).sum(),\n",
    "        '4': (kmeans.labels_ == 4).sum(),\n",
    "    }\n",
    "}\n",
    "\n",
    "print(cluster_data['counts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with connection.cursor() as cursor:\n",
    "    cursor.execute(\"DROP TABLE IF EXISTS clusters;\")\n",
    "    cursor.execute(\"CREATE TABLE clusters(id INTEGER, centroid double precision[]);\")\n",
    "    for i, centroid in enumerate(cluster_data['centroids']):\n",
    "        cursor.execute(\"INSERT INTO clusters(id, centroid) VALUES (%s, %s)\", (i, centroid))\n",
    "    connection.commit()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a cluster map we can add a cluster index to the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with connection.cursor() as cursor:\n",
    "    cursor.execute(\"DROP TABLE IF EXISTS clustered_dataset;\")\n",
    "    cursor.execute(\"CREATE TABLE clustered_dataset AS SELECT * from dataset;\")\n",
    "    cursor.execute(\"ALTER TABLE clustered_dataset ADD COLUMN cluster INTEGER;\")\n",
    "\n",
    "    for i, cluster in enumerate(cluster_data['clusters']):\n",
    "        cursor.execute(\"UPDATE clustered_dataset SET cluster=%s WHERE id=%s\", (cluster, i+1))\n",
    "\n",
    "    connection.commit()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now querying data happens in 2 steps\n",
    "1. Find the cluster that the query text belongs to\n",
    "2. Find the nearest neighbors within that cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered_test_query = \"\"\"\n",
    "    EXPLAIN ANALYZE WITH target_item AS (\n",
    "    SELECT id, embeddings, cluster\n",
    "    FROM clustered_dataset\n",
    "    WHERE id = 60\n",
    "    ),\n",
    "    nearest_centroid AS (\n",
    "    SELECT c.id AS centroid_id, c.centroid\n",
    "    FROM clusters c\n",
    "    ORDER BY cosine_distance(c.centroid, (SELECT embeddings FROM target_item)) DESC\n",
    "    LIMIT 1\n",
    "    )\n",
    "    SELECT ds.id, ds.text\n",
    "    FROM clustered_dataset ds\n",
    "    WHERE ds.cluster = (SELECT centroid_id FROM nearest_centroid)\n",
    "    AND ds.id <> 60\n",
    "    ORDER BY cosine_distance(ds.embeddings, (SELECT embeddings FROM target_item)) DESC\n",
    "    LIMIT 20;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Limit  (cost=519.43..519.48 rows=20 width=138) (actual time=96.114..96.122 rows=20 loops=1)\n",
      "  CTE target_item\n",
      "    ->  Seq Scan on clustered_dataset  (cost=0.00..57.50 rows=1 width=26) (actual time=0.364..0.510 rows=1 loops=1)\n",
      "          Filter: (id = 60)\n",
      "          Rows Removed by Filter: 999\n",
      "  CTE nearest_centroid\n",
      "    ->  Limit  (cost=346.57..346.57 rows=1 width=44) (actual time=4.475..4.477 rows=1 loops=1)\n",
      "          InitPlan 2 (returns $1)\n",
      "            ->  CTE Scan on target_item  (cost=0.00..0.02 rows=1 width=32) (actual time=0.366..0.512 rows=1 loops=1)\n",
      "          ->  Sort  (cost=346.55..349.73 rows=1270 width=44) (actual time=4.474..4.475 rows=1 loops=1)\n",
      "                Sort Key: (cosine_distance(c.centroid, $1)) DESC\n",
      "                Sort Method: top-N heapsort  Memory: 25kB\n",
      "                ->  Seq Scan on clusters c  (cost=0.00..340.20 rows=1270 width=44) (actual time=2.169..4.463 rows=5 loops=1)\n",
      "  InitPlan 4 (returns $3)\n",
      "    ->  CTE Scan on target_item target_item_1  (cost=0.00..0.02 rows=1 width=32) (actual time=0.001..0.002 rows=1 loops=1)\n",
      "  InitPlan 5 (returns $4)\n",
      "    ->  CTE Scan on nearest_centroid  (cost=0.00..0.02 rows=1 width=4) (actual time=4.478..4.479 rows=1 loops=1)\n",
      "  ->  Sort  (cost=115.32..115.82 rows=200 width=138) (actual time=96.112..96.114 rows=20 loops=1)\n",
      "        Sort Key: (cosine_distance(ds.embeddings, $3)) DESC\n",
      "        Sort Method: top-N heapsort  Memory: 28kB\n",
      "        ->  Seq Scan on clustered_dataset ds  (cost=0.00..110.00 rows=200 width=138) (actual time=5.078..96.005 rows=159 loops=1)\n",
      "              Filter: ((id <> 60) AND (cluster = $4))\n",
      "              Rows Removed by Filter: 841\n",
      "Planning Time: 0.639 ms\n",
      "Execution Time: 96.342 ms\n"
     ]
    }
   ],
   "source": [
    "with connection.cursor() as cursor:\n",
    "    cursor.execute(clustered_test_query)\n",
    "    for line in cursor.fetchall():\n",
    "        print(line[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This has greatly improved the performance. We've been able to eliminate the need to scan the entire table (it's only looking at 563 rows). The performance here will be impacted by the number of clusters you are able to generate. however this does mean that you may be excluding results that are somewhat close but in neibooring clusters. One approach to mitigate this is to query the nearest neighbors from the neighboring clusters as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "with connection.cursor() as cursor:\n",
    "    cursor.execute(\"DROP INDEX IF EXISTS clustered_dataset_pkey;\")\n",
    "    cursor.execute(\"DROP INDEX IF EXISTS idx_clustered_dataset_cluster;\")\n",
    "    cursor.execute(\"DROP INDEX IF EXISTS idx_clusters_centroid;\")\n",
    "    cursor.execute(\"CREATE UNIQUE INDEX clustered_dataset_pkey ON clustered_dataset(id int4_ops);\")\n",
    "    cursor.execute(\"CREATE INDEX idx_clustered_dataset_cluster ON clustered_dataset (cluster);\")\n",
    "    cursor.execute(\"CREATE INDEX idx_clusters_centroid ON clusters USING GIN (centroid);\")\n",
    "    connection.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Limit  (cost=104.43..104.48 rows=20 width=138) (actual time=97.251..97.258 rows=20 loops=1)\n",
      "  CTE target_item\n",
      "    ->  Index Scan using clustered_dataset_pkey on clustered_dataset  (cost=0.28..2.49 rows=1 width=26) (actual time=0.031..0.032 rows=1 loops=1)\n",
      "          Index Cond: (id = 60)\n",
      "  CTE nearest_centroid\n",
      "    ->  Limit  (cost=2.34..2.35 rows=1 width=44) (actual time=3.091..3.093 rows=1 loops=1)\n",
      "          InitPlan 2 (returns $1)\n",
      "            ->  CTE Scan on target_item  (cost=0.00..0.02 rows=1 width=32) (actual time=0.033..0.034 rows=1 loops=1)\n",
      "          ->  Sort  (cost=2.32..2.34 rows=5 width=44) (actual time=3.090..3.091 rows=1 loops=1)\n",
      "                Sort Key: (cosine_distance(c.centroid, $1)) DESC\n",
      "                Sort Method: top-N heapsort  Memory: 25kB\n",
      "                ->  Seq Scan on clusters c  (cost=0.00..2.30 rows=5 width=44) (actual time=0.684..3.082 rows=5 loops=1)\n",
      "  InitPlan 4 (returns $3)\n",
      "    ->  CTE Scan on target_item target_item_1  (cost=0.00..0.02 rows=1 width=32) (actual time=0.001..0.002 rows=1 loops=1)\n",
      "  InitPlan 5 (returns $4)\n",
      "    ->  CTE Scan on nearest_centroid  (cost=0.00..0.02 rows=1 width=4) (actual time=3.093..3.093 rows=1 loops=1)\n",
      "  ->  Sort  (cost=99.55..100.05 rows=200 width=138) (actual time=97.249..97.251 rows=20 loops=1)\n",
      "        Sort Key: (cosine_distance(ds.embeddings, $3)) DESC\n",
      "        Sort Method: top-N heapsort  Memory: 28kB\n",
      "        ->  Index Scan using idx_clustered_dataset_cluster on clustered_dataset ds  (cost=0.15..94.23 rows=200 width=138) (actual time=3.748..97.148 rows=159 loops=1)\n",
      "              Index Cond: (cluster = $4)\n",
      "              Filter: (id <> 60)\n",
      "Planning Time: 0.392 ms\n",
      "Execution Time: 97.332 ms\n"
     ]
    }
   ],
   "source": [
    "with connection.cursor() as cursor:\n",
    "    cursor.execute(clustered_test_query)\n",
    "    for line in cursor.fetchall():\n",
    "        print(line[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding indexes allows us to replace some sequential scans with index scans. This isn't that big of an improvement here but would be more significant with a larger dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the pgvector extension to create a GIN index on the embeddings column. This will allow us to use the `pgvector <-> pgvector` operator to find the nearest neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "with connection.cursor() as cursor:\n",
    "    cursor.execute(\"DROP TABLE IF EXISTS vectorized_dataset;\")\n",
    "    cursor.execute(\"CREATE EXTENSION IF NOT EXISTS vector WITH SCHEMA extensions;\");\n",
    "    cursor.execute(\"CREATE TABLE vectorized_dataset AS SELECT * from dataset;\")\n",
    "    \n",
    "    # 768 seems to be the default length for the instructor-large model\n",
    "    cursor.execute(\"ALTER TABLE vectorized_dataset ADD COLUMN embeddings_vector vector(768);\")\n",
    "    connection.commit()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's copy over our data to the new table and see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "with connection.cursor() as cursor:\n",
    "    cursor.execute(\"SELECT embeddings FROM vectorized_dataset;\")\n",
    "    embeddings = cursor.fetchall()\n",
    "    embeddings = [e[0] for e in embeddings]\n",
    "    for i, embedding in enumerate(embeddings):\n",
    "        cursor.execute(\"UPDATE vectorized_dataset SET embeddings_vector=%s WHERE id=%s\", (embedding, i+1))\n",
    "    connection.commit()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_test_query = \"\"\"\n",
    "    WITH target_item AS (\n",
    "        SELECT * \n",
    "        FROM vectorized_dataset \n",
    "        WHERE id=60\n",
    "    )\n",
    "    SELECT ds.text, ds.embeddings_vector <=> target_item.embeddings_vector as score\n",
    "    FROM vectorized_dataset ds, target_item\n",
    "    ORDER BY score DESC\n",
    "    LIMIT 20;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.242681830496018: CHORUS. Now old desire doth in his deathbed lie, And young affection gapes to be his heir; That fair for which love groan’d for and would die, With tender Juliet match’d, is now not fair. Now Romeo is belov’d, and loves again, Alike bewitched by the charm of looks; But to his foe suppos’d he must complain, And she steal love’s sweet bait from fearful hooks: Being held a foe, he may not have access To breathe such vows as lovers use to swear; And she as much in love, her means much less To meet her new beloved anywhere. But passion lends them power, time means, to meet, Tempering extremities with extreme sweet.\n",
      "0.242681830496018: ROMEO. Yet banished? Hang up philosophy. Unless philosophy can make a Juliet, Displant a town, reverse a Prince’s doom, It helps not, it prevails not, talk no more.\n",
      "0.242681830496018: TYBALT. This by his voice, should be a Montague. Fetch me my rapier, boy. What, dares the slave Come hither, cover’d with an antic face, To fleer and scorn at our solemnity? Now by the stock and honour of my kin, To strike him dead I hold it not a sin.\n",
      "0.242681830496018: ROMEO. O, she doth teach the torches to burn bright! It seems she hangs upon the cheek of night As a rich jewel in an Ethiop’s ear; Beauty too rich for use, for earth too dear! So shows a snowy dove trooping with crows As yonder lady o’er her fellows shows. The measure done, I’ll watch her place of stand, And touching hers, make blessed my rude hand. Did my heart love till now? Forswear it, sight! For I ne’er saw true beauty till this night.\n",
      "0.242681830496018: CAPULET. Content thee, gentle coz, let him alone, A bears him like a portly gentleman; And, to say truth, Verona brags of him To be a virtuous and well-govern’d youth. I would not for the wealth of all the town Here in my house do him disparagement. Therefore be patient, take no note of him, It is my will; the which if thou respect, Show a fair presence and put off these frowns, An ill-beseeming semblance for a feast.\n",
      "0.242681830496018: CAPULET. Nay, gentlemen, prepare not to be gone, We have a trifling foolish banquet towards. Is it e’en so? Why then, I thank you all; I thank you, honest gentlemen; good night. More torches here! Come on then, let’s to bed. Ah, sirrah, by my fay, it waxes late, I’ll to my rest.\n",
      "0.242681830496018: SERVANT. I know not, sir.\n",
      "0.242681830496018: CAPULET. My sword, I say! Old Montague is come, And flourishes his blade in spite of me.\n",
      "0.242681830496018: JULIET. And stint thou too, I pray thee, Nurse, say I.\n",
      "0.242681830496018: CHORUS. Two households, both alike in dignity, In fair Verona, where we lay our scene, From ancient grudge break to new mutiny, Where civil blood makes civil hands unclean. From forth the fatal loins of these two foes A pair of star-cross’d lovers take their life; Whose misadventur’d piteous overthrows Doth with their death bury their parents’ strife. The fearful passage of their death-mark’d love, And the continuance of their parents’ rage, Which, but their children’s end, nought could remove, Is now the two hours’ traffic of our stage; The which, if you with patient ears attend, What here shall miss, our toil shall strive to mend.\n",
      "0.242681830496018: BENVOLIO. This wind you talk of blows us from ourselves: Supper is done, and we shall come too late.\n",
      "0.242681830496018: ROMEO. What lady is that, which doth enrich the hand Of yonder knight?\n",
      "0.242681830496018: CAPULET. What, man, ’tis not so much, ’tis not so much: ’Tis since the nuptial of Lucentio, Come Pentecost as quickly as it will, Some five and twenty years; and then we mask’d.\n",
      "0.242681830496018: CAPULET. Go to, go to! You are a saucy boy. Is’t so, indeed? This trick may chance to scathe you, I know what. You must contrary me! Marry, ’tis time. Well said, my hearts!—You are a princox; go: Be quiet, or—More light, more light!—For shame! I’ll make you quiet. What, cheerly, my hearts.\n",
      "0.242681830496018: CAPULET. But Montague is bound as well as I, In penalty alike; and ’tis not hard, I think, For men so old as we to keep the peace.\n",
      "0.242681830496018: CAPULET. But saying o’er what I have said before. My child is yet a stranger in the world, She hath not seen the change of fourteen years; Let two more summers wither in their pride Ere we may think her ripe to be a bride.\n",
      "0.242681830496018: CAPULET. What noise is this? Give me my long sword, ho!\n",
      "0.242681830496018: CAPULET, head of a Veronese family at feud with the Montagues. LADY CAPULET, wife to Capulet. JULIET, daughter to Capulet. TYBALT, nephew to Lady Capulet. CAPULET’S COUSIN, an old man. NURSE to Juliet. PETER, servant to Juliet’s Nurse. SAMPSON, servant to Capulet. GREGORY, servant to Capulet. Servants.\n",
      "0.242681830496018: CAPULET. And too soon marr’d are those so early made. The earth hath swallowed all my hopes but she, She is the hopeful lady of my earth: But woo her, gentle Paris, get her heart, My will to her consent is but a part; And she agree, within her scope of choice Lies my consent and fair according voice. This night I hold an old accustom’d feast, Whereto I have invited many a guest, Such as I love, and you among the store, One more, most welcome, makes my number more. At my poor house look to behold this night Earth-treading stars that make dark heaven light: Such comfort as do lusty young men feel When well apparell’d April on the heel Of limping winter treads, even such delight Among fresh female buds shall you this night Inherit at my house. Hear all, all see, And like her most whose merit most shall be: Which, on more view of many, mine, being one, May stand in number, though in reckoning none. Come, go with me. Go, sirrah, trudge about Through fair Verona; find those persons out Whose names are written there, [_gives a paper_] and to them say, My house and welcome on their pleasure stay.\n",
      "0.242681830496018: CAPULET. Monday! Ha, ha! Well, Wednesday is too soon, A Thursday let it be; a Thursday, tell her, She shall be married to this noble earl. Will you be ready? Do you like this haste? We’ll keep no great ado,—a friend or two, For, hark you, Tybalt being slain so late, It may be thought we held him carelessly, Being our kinsman, if we revel much. Therefore we’ll have some half a dozen friends, And there an end. But what say you to Thursday?\n"
     ]
    }
   ],
   "source": [
    "with connection.cursor() as cursor:\n",
    "    cursor.execute(vectorized_test_query)\n",
    "    for line in cursor.fetchall():\n",
    "        text = line[0].strip().replace(\"\\n\", \" \")\n",
    "        score = line[1]\n",
    "        print(f\"{score}: {text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Limit  (cost=155.61..155.66 rows=20 width=134) (actual time=7.986..7.991 rows=20 loops=1)\n",
      "  ->  Sort  (cost=155.61..158.11 rows=1000 width=134) (actual time=7.985..7.987 rows=20 loops=1)\n",
      "        Sort Key: ((ds.embeddings_vector <=> vectorized_dataset.embeddings_vector)) DESC\n",
      "        Sort Method: top-N heapsort  Memory: 35kB\n",
      "        ->  Nested Loop  (cost=0.00..129.00 rows=1000 width=134) (actual time=0.071..7.667 rows=1000 loops=1)\n",
      "              ->  Seq Scan on vectorized_dataset  (cost=0.00..59.50 rows=1 width=18) (actual time=0.044..0.144 rows=1 loops=1)\n",
      "                    Filter: (id = 60)\n",
      "                    Rows Removed by Filter: 999\n",
      "              ->  Seq Scan on vectorized_dataset ds  (cost=0.00..57.00 rows=1000 width=144) (actual time=0.002..0.119 rows=1000 loops=1)\n",
      "Planning Time: 0.096 ms\n",
      "Execution Time: 8.032 ms\n"
     ]
    }
   ],
   "source": [
    "with connection.cursor() as cursor:\n",
    "    cursor.execute(\"EXPLAIN ANALYZE\" + vectorized_test_query)\n",
    "    for line in cursor.fetchall():\n",
    "        print(line[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance difference is staggering. We've gone from 333.352 ms to 8.112 ms. That's a 41x improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "with connection.cursor() as cursor:\n",
    "    cursor.execute(\"DROP TABLE IF EXISTS indexed_vectorized_dataset;\")\n",
    "    cursor.execute(\"DROP INDEX IF EXISTS idx_indexed_vectorized_dataset_pkey;\")\n",
    "    cursor.execute(\n",
    "        \"DROP INDEX IF EXISTS idx_indexed_vectorized_dataset_embeddings_vector_idx;\"\n",
    "    )\n",
    "\n",
    "    cursor.execute(\n",
    "        \"CREATE TABLE indexed_vectorized_dataset AS SELECT * from vectorized_dataset;\")\n",
    "    cursor.execute(\n",
    "        \"CREATE INDEX idx_indexed_vectorized_dataset_pkey ON indexed_vectorized_dataset(id int4_ops);\")\n",
    "    cursor.execute(\n",
    "        \"CREATE INDEX idx_indexed_vectorized_dataset_embeddings_vector ON indexed_vectorized_dataset USING ivfflat (embeddings_vector vector_cosine_ops) WITH (lists = 100);\"\n",
    "    )\n",
    "\n",
    "    connection.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexed_vectorized_test_query = \"\"\"\n",
    "    WITH target_item AS (\n",
    "        SELECT * \n",
    "        FROM indexed_vectorized_dataset \n",
    "        WHERE id=60\n",
    "    )\n",
    "    SELECT ds.text, ds.embeddings_vector <=> target_item.embeddings_vector as score\n",
    "    FROM indexed_vectorized_dataset ds, target_item\n",
    "    ORDER BY score DESC\n",
    "    LIMIT 20;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Limit  (cost=10000000076.60..10000000076.65 rows=20 width=134) (actual time=8.033..8.036 rows=20 loops=1)\n",
      "  ->  Sort  (cost=10000000076.60..10000000079.10 rows=1000 width=134) (actual time=8.031..8.033 rows=20 loops=1)\n",
      "        Sort Key: ((ds.embeddings_vector <=> indexed_vectorized_dataset.embeddings_vector)) DESC\n",
      "        Sort Method: top-N heapsort  Memory: 35kB\n",
      "        ->  Nested Loop  (cost=10000000000.27..10000000049.99 rows=1000 width=134) (actual time=0.045..7.734 rows=1000 loops=1)\n",
      "              ->  Index Scan using idx_indexed_vectorized_dataset_pkey on indexed_vectorized_dataset  (cost=0.28..2.49 rows=1 width=18) (actual time=0.012..0.013 rows=1 loops=1)\n",
      "                    Index Cond: (id = 60)\n",
      "              ->  Seq Scan on indexed_vectorized_dataset ds  (cost=10000000000.00..10000000035.00 rows=1000 width=144) (actual time=0.008..0.192 rows=1000 loops=1)\n",
      "Planning Time: 0.117 ms\n",
      "Execution Time: 8.080 ms\n"
     ]
    }
   ],
   "source": [
    "with connection.cursor() as cursor:\n",
    "    cursor.execute(\"SET enable_seqscan=false;\")\n",
    "    connection.commit()\n",
    "    cursor.execute(\"EXPLAIN ANALYZE\" + indexed_vectorized_test_query)\n",
    "    for line in cursor.fetchall():\n",
    "        print(line[0])\n",
    "    cursor.execute(\"SET enable_seqscan=true;\")\n",
    "    connection.commit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
